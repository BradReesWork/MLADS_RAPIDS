{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.41\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script sh --out /dev/null --err /dev/null\n",
    "cd .. && mkdir -p data/mortgage_2000 && cd data/mortgage_2000 && wget http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names.csv\n",
      "acq/Acquisition_2000Q4.txt\n",
      "acq/Acquisition_2000Q3.txt\n",
      "acq/Acquisition_2000Q2.txt\n",
      "acq/Acquisition_2000Q1.txt\n",
      "perf/Performance_2000Q4.txt\n",
      "perf/Performance_2000Q3.txt\n",
      "perf/Performance_2000Q2.txt\n",
      "perf/Performance_2000Q1.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "cd ../data/mortgage_2000 && tar -xvf mortgage_2000.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = os.environ.get(\"SUBSCRIPTION_ID\", \"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\")\n",
    "resource_group = os.environ.get(\"RESOURCE_GROUP\", \"MLADS_todrabas\")\n",
    "workspace_name = os.environ.get(\"WORKSPACE_NAME\", \"todrabas_MLADS_WE\")\n",
    "# workspace_region = os.environ.get(\"WORKSPACE_REGION\", \"\")\n",
    "\n",
    "ws = Workspace(workspace_name=workspace_name, subscription_id=subscription_id, resource_group=resource_group)\n",
    "\n",
    "# write config to a local directory for future use\n",
    "# ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.write_config(path='./config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config[\"SUBSCRIPTION_ID\"] = \"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\"\n",
    "config[\"RESOURCE_GROUP\"] = \"MLADS_todrabas\"\n",
    "config[\"WORKSPACE_NAME\"] = \"todrabas_MLADS_WE\"\n",
    "config[\"GPU_CLUSTER_NAME\"] = \"gpu-todrabas\"\n",
    "\n",
    "with open('config/config.json', 'w') as f:\n",
    "    f.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_folder = \"scripts\"\n",
    "\n",
    "# import shutil\n",
    "# shutil.copy('./1_pandasVsRapids_ETL.py', os.path.join(scripts_folder, '1_pandasVsRapids_ETL.py'))\n",
    "\n",
    "# with open(os.path.join(scripts_folder, './1_pandasVsRapids_ETL.py'), 'r') as process_data_script:\n",
    "#     print(process_data_script.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new cluster\n",
      "Creating\n",
      "Succeeded..............\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "gpu_cluster_name = \"gpu-todrabas\"\n",
    "\n",
    "if gpu_cluster_name in ws.compute_targets:\n",
    "    gpu_cluster = ws.compute_targets[gpu_cluster_name]\n",
    "    \n",
    "    if gpu_cluster and type(gpu_cluster) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + gpu_cluster_name)\n",
    "else:\n",
    "    print(\"creating new cluster\")\n",
    "    # vm_size parameter below could be modified to one of the RAPIDS-supported VM types\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"Standard_NC6s_v2\", min_nodes=1, max_nodes = 1)\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import hashlib\n",
    "from urllib.request import urlretrieve\n",
    "# from progressbar import ProgressBar\n",
    "\n",
    "def validate_downloaded_data(path):\n",
    "    if(os.path.isdir(path) and os.path.exists(path + '//names.csv')) :\n",
    "        if(os.path.isdir(path + '//acq' ) and len(os.listdir(path + '//acq')) == 8):\n",
    "            if(os.path.isdir(path + '//perf' ) and len(os.listdir(path + '//perf')) == 11):\n",
    "                print(\"Data has been downloaded and decompressed at: {0}\".format(path))\n",
    "                return True\n",
    "    print(\"Data has not been downloaded and decompressed\")\n",
    "    return False\n",
    "\n",
    "# def show_progress(count, block_size, total_size):\n",
    "#     global pbar\n",
    "#     global processed\n",
    "    \n",
    "#     if count == 0:\n",
    "#         pbar = ProgressBar(maxval=total_size)\n",
    "#         processed = 0\n",
    "    \n",
    "#     processed += block_size\n",
    "#     processed = min(processed,total_size)\n",
    "#     pbar.update(processed)\n",
    "\n",
    "        \n",
    "def download_file(fileroot):\n",
    "    filename = fileroot + '.tgz'\n",
    "    if(not os.path.exists(filename) or hashlib.md5(open(filename, 'rb').read()).hexdigest() != '82dd47135053303e9526c2d5c43befd5' ):\n",
    "        url_format = 'http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/{0}.tgz'\n",
    "        url = url_format.format(fileroot)\n",
    "        print(\"...Downloading file :{0}\".format(filename))\n",
    "        urlretrieve(url, filename)\n",
    "#         pbar.finish()\n",
    "        print(\"...File :{0} finished downloading\".format(filename))\n",
    "    else:\n",
    "        print(\"...File :{0} has been downloaded already\".format(filename))\n",
    "    return filename\n",
    "\n",
    "def decompress_file(filename,path):\n",
    "    tar = tarfile.open(filename)\n",
    "    print(\"...Getting information from {0} about files to decompress\".format(filename))\n",
    "    members = tar.getmembers()\n",
    "    numFiles = len(members)\n",
    "    so_far = 0\n",
    "    for member_info in members:\n",
    "        tar.extract(member_info,path=path)\n",
    "#         show_progress(so_far, 1, numFiles)\n",
    "        so_far += 1\n",
    "#     pbar.finish()\n",
    "    print(\"...All {0} files have been decompressed\".format(numFiles))\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\mortgage_2000\n",
      "Data has not been downloaded and decompressed\n",
      "Downloading and Decompressing Input Data\n",
      "...Downloading file :mortgage_2000.tgz\n",
      "...File :mortgage_2000.tgz finished downloading\n",
      "...Getting information from mortgage_2000.tgz about files to decompress\n",
      "...All 9 files have been decompressed\n",
      "Input Data has been Downloaded and Decompressed\n"
     ]
    }
   ],
   "source": [
    "fileroot = 'mortgage_2000'\n",
    "path = '.\\\\{0}'.format(fileroot)\n",
    "pbar = None\n",
    "processed = 0\n",
    "\n",
    "print(path)\n",
    "if(not validate_downloaded_data(path)):\n",
    "    print(\"Downloading and Decompressing Input Data\")\n",
    "    filename = download_file(fileroot)\n",
    "    decompress_file(filename,path)\n",
    "    print(\"Input Data has been Downloaded and Decompressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 459928\n",
      "drwxrwxr-x 6 1002 1002      4096 Jun  3 17:22 .\n",
      "drwxrwxr-x 6 1002 1002      4096 May 28 17:50 ..\n",
      "drwxr-xr-x 4 root root      4096 May 31 18:22 .\\mortgage_2000\n",
      "drwxrwxr-x 2 1002 1002      4096 May 31 18:40 .ipynb_checkpoints\n",
      "-rw-rw-r-- 1 1002 1002     43804 May 30 17:39 1_pandasVsRapids_ETL.ipynb\n",
      "-rw-r--r-- 1 root root      5477 Jun  3 17:04 2_pandasVsRapids_DBSCAN.ipynb\n",
      "-rw-r--r-- 1 root root    294388 May 30 18:05 3_Rapids_flow_classification.ipynb\n",
      "-rw-rw-r-- 1 1002 1002     29883 Jun  3 17:22 4_Rapids_AzureML_ETL.ipynb\n",
      "-rw-r--r-- 1 root root 470557209 Jun  3 17:21 mortgage_2000.tgz\n",
      "drwxr-xr-x 2 root root      4096 Jun  3 17:17 mortgage_np\n",
      "drwxr-xr-x 3 root root      4096 May 31 18:58 scripts\n"
     ]
    }
   ],
   "source": [
    "!ls -la ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading .\\mortgage_2000/acq/Acquisition_2000Q1.txt\n",
      "Uploading .\\mortgage_2000/acq/Acquisition_2000Q2.txt\n",
      "Uploading .\\mortgage_2000/acq/Acquisition_2000Q3.txt\n",
      "Uploading .\\mortgage_2000/acq/Acquisition_2000Q4.txt\n",
      "Uploading .\\mortgage_2000/names.csv\n",
      "Uploading .\\mortgage_2000/perf/Performance_2000Q1.txt\n",
      "Uploading .\\mortgage_2000/perf/Performance_2000Q2.txt\n",
      "Uploading .\\mortgage_2000/perf/Performance_2000Q3.txt\n",
      "Uploading .\\mortgage_2000/perf/Performance_2000Q4.txt\n",
      "Uploaded .\\mortgage_2000/names.csv, 1 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/acq/Acquisition_2000Q1.txt, 2 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/acq/Acquisition_2000Q2.txt, 3 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/acq/Acquisition_2000Q4.txt, 4 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/acq/Acquisition_2000Q3.txt, 5 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/perf/Performance_2000Q2.txt, 6 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/perf/Performance_2000Q3.txt, 7 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/perf/Performance_2000Q4.txt, 8 files out of an estimated total of 9\n",
      "Uploaded .\\mortgage_2000/perf/Performance_2000Q1.txt, 9 files out of an estimated total of 9\n"
     ]
    }
   ],
   "source": [
    "# fileroot = 'data\\mortagage_2000'\n",
    "# path = '.\\\\{0}'.format(fileroot)\n",
    "# print(path)\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# download and uncompress data in a local directory before uploading to data store\n",
    "# directory specified in src_dir parameter below should have the acq, perf directories with data and names.csv file\n",
    "ds.upload(src_dir=path, target_path=fileroot, overwrite=True, show_progress=True)\n",
    "\n",
    "# data already uploaded to the datastore\n",
    "data_ref = DataReference(data_reference_name='data', datastore=ds, path_on_datastore=fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Downloading file :mortgage.npy.gz\n",
      "...File :mortgage.npy.gz finished downloading\n"
     ]
    }
   ],
   "source": [
    "filename = 'mortgage.npy.gz'\n",
    "url = 'https://github.com/rapidsai/notebooks-extended/raw/master/data/mortgage/mortgage.npy.gz'\n",
    "\n",
    "print(\"...Downloading file :{0}\".format(filename))\n",
    "urlretrieve(url, filename)\n",
    "print(\"...File :{0} finished downloading\".format(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading mortgage_np/.ipynb_checkpoints/mortgage-checkpoint.csv\n",
      "Uploading mortgage_np/mortgage.csv\n",
      "Uploading mortgage_np/mortgage.npy.gz\n",
      "Uploaded mortgage_np/.ipynb_checkpoints/mortgage-checkpoint.csv, 1 files out of an estimated total of 3\n",
      "Uploaded mortgage_np/mortgage.npy.gz, 2 files out of an estimated total of 3\n",
      "Uploaded mortgage_np/mortgage.csv, 3 files out of an estimated total of 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_4fe7bbde9f7c4d149c691834fae9f4bc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'mortgage_np'\n",
    "froot = 'mortgage_np'\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# download and uncompress data in a local directory before uploading to data store\n",
    "# directory specified in src_dir parameter below should have the acq, perf directories with data and names.csv file\n",
    "ds.upload(src_dir=path, target_path=froot, overwrite=True, show_progress=True)\n",
    "\n",
    "# # data already uploaded to the datastore\n",
    "# data_ref = DataReference(data_reference_name='data', datastore=ds, path_on_datastore=froot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Downloading file :unswiotflow.tar.gz\n",
      "...File :unswiotflow.tar.gz finished downloading\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import hashlib\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "filename = 'unswiotflow.tar.gz'\n",
    "url = 'https://github.com/rapidsai/notebooks-extended/blob/master/data/unswiot/unswiotflow.tar.gz?raw=true'\n",
    "\n",
    "print(\"...Downloading file :{0}\".format(filename))\n",
    "urlretrieve(url, filename)\n",
    "print(\"...File :{0} finished downloading\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Getting information from unswiotflow.tar.gz about files to decompress\n",
      "...All 3 files have been decompressed\n"
     ]
    }
   ],
   "source": [
    "path = 'unswiot'\n",
    "decompress_file(filename,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading unswiot/conn.log\n",
      "Uploading unswiot/lab_mac_labels_cats.csv\n",
      "Uploading unswiot/small_sample.pcap\n",
      "Uploaded unswiot/lab_mac_labels_cats.csv, 1 files out of an estimated total of 3\n",
      "Uploaded unswiot/small_sample.pcap, 2 files out of an estimated total of 3\n",
      "Uploaded unswiot/conn.log, 3 files out of an estimated total of 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_b3e57f3ce3c840d9a4fc373290c23006"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = 'mortgage_np'\n",
    "froot = path\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# download and uncompress data in a local directory before uploading to data store\n",
    "# directory specified in src_dir parameter below should have the acq, perf directories with data and names.csv file\n",
    "ds.upload(src_dir=path, target_path=froot, overwrite=True, show_progress=True)\n",
    "\n",
    "# # data already uploaded to the datastore\n",
    "# data_ref = DataReference(data_reference_name='data', datastore=ds, path_on_datastore=froot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.framework = 'python'\n",
    "run_config.environment.python.user_managed_dependencies = True\n",
    "run_config.environment.python.interpreter_path = '/conda/envs/rapids/bin/python'\n",
    "run_config.target = gpu_cluster_name\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.gpu_support = True\n",
    "run_config.environment.docker.base_image = \"rapidsai/rapidsai:cuda9.2-runtime-ubuntu18.04\"\n",
    "# run_config.environment.docker.base_image_registry.address = '<registry_url>' # not required if the base_image is in Docker hub\n",
    "# run_config.environment.docker.base_image_registry.username = '<user_name>' # needed only for private images\n",
    "# run_config.environment.docker.base_image_registry.password = '<password>' # needed only for private images\n",
    "run_config.environment.spark.precache_packages = False\n",
    "run_config.data_references={'data':data_ref.to_config()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: rapidstest_cpu_1559329102_76a4e0f3\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/MLADS_todrabas/providers/Microsoft.MachineLearningServices/workspaces/todrabas_MLADS_WE/experiments/rapidstest_cpu/runs/rapidstest_cpu_1559329102_76a4e0f3\n",
      "\n",
      "Streaming azureml-logs/80_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Warning: Couldn't instantiate AppInsights telemetry client. Telemetry disabled.\n",
      "Warning: Unable to import azureml.history. Output collection disabled.\n",
      "Running ETL...\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/todrabas_mlads_we/azureml/rapidstest_cpu_1559329102_76a4e0f3/mounts/workspaceblobstore/mortgage_2000/acq/Acquisition_2000Q1.txt\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/todrabas_mlads_we/azureml/rapidstest_cpu_1559329102_76a4e0f3/mounts/workspaceblobstore/mortgage_2000/perf/Performance_2000Q1.txt\n",
      "Creating ever delinquent statuses...\n",
      "Creating delinquency statuses...\n",
      "1_pandasVsRapids_ETL.py:475: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test['timestamp'] = test['monthly_reporting_period']\n",
      "\tProcessing month: 1\n",
      "1_pandasVsRapids_ETL.py:458: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  tmpdf['josh_months'] = tmpdf['timestamp_year'] * 12 + tmpdf['timestamp_month']\n",
      "1_pandasVsRapids_ETL.py:459: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  tmpdf['josh_mody_n'] = ((tmpdf['josh_months'].astype('float64') - 24000 - y) / 12).apply(np.floor)\n",
      "\tProcessing month: 2\n",
      "\tProcessing month: 3\n",
      "\tProcessing month: 4\n",
      "\tProcessing month: 5\n",
      "\tProcessing month: 6\n",
      "\tProcessing month: 7\n",
      "\tProcessing month: 8\n",
      "\tProcessing month: 9\n",
      "\tProcessing month: 10\n",
      "\tProcessing month: 11\n",
      "\tProcessing month: 12\n",
      "dropping: loan_id\n",
      "dropping: orig_date\n",
      "dropping: first_pay_date\n",
      "dropping: seller_name\n",
      "dropping: monthly_reporting_period\n",
      "dropping: last_paid_installment_date\n",
      "dropping: maturity_date\n",
      "dropping: ever_30\n",
      "dropping: ever_90\n",
      "dropping: ever_180\n",
      "dropping: delinquency_30\n",
      "dropping: delinquency_90\n",
      "dropping: delinquency_180\n",
      "dropping: upb_12\n",
      "dropping: zero_balance_effective_date\n",
      "dropping: foreclosed_after\n",
      "dropping: disposition_date\n",
      "dropping: timestamp\n",
      "servicer object\n",
      "interest_rate float64\n",
      "current_actual_upb float64\n",
      "loan_age float64\n",
      "remaining_months_to_legal_maturity float64\n",
      "adj_remaining_months_to_maturity float64\n",
      "msa float64\n",
      "current_loan_delinquency_status int64\n",
      "mod_flag object\n",
      "zero_balance_code float64\n",
      "foreclosure_costs float64\n",
      "prop_preservation_and_repair_costs float64\n",
      "asset_recovery_costs float64\n",
      "misc_holding_expenses float64\n",
      "holding_taxes float64\n",
      "net_sale_proceeds float64\n",
      "credit_enhancement_proceeds float64\n",
      "repurchase_make_whole_proceeds float64\n",
      "other_foreclosure_proceeds float64\n",
      "non_interest_bearing_upb float64\n",
      "principal_forgiveness_upb float64\n",
      "repurchase_make_whole_proceeds_flag object\n",
      "foreclosure_principal_write_off_amount float64\n",
      "servicing_activity_indicator object\n",
      "delinquency_12 int32\n",
      "orig_channel object\n",
      "orig_interest_rate float64\n",
      "orig_upb float64\n",
      "orig_loan_term float64\n",
      "orig_ltv float64\n",
      "orig_cltv float64\n",
      "num_borrowers float64\n",
      "dti float64\n",
      "borrower_credit_score float64\n",
      "first_home_buyer object\n",
      "loan_purpose object\n",
      "property_type object\n",
      "num_units float64\n",
      "occupancy_status object\n",
      "property_state object\n",
      "zip float64\n",
      "mortgage_insurance_percent float64\n",
      "product_type object\n",
      "coborrow_credit_score float64\n",
      "mortgage_insurance_type float64\n",
      "relocation_mortgage_indicator object\n",
      "new object\n",
      "Total ETL Time: 0:16:46.866488\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Warning: Couldn't instantiate AppInsights telemetry client. Telemetry disabled.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: rapidstest_cpu_1559329102_76a4e0f3\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/MLADS_todrabas/providers/Microsoft.MachineLearningServices/workspaces/todrabas_MLADS_WE/experiments/rapidstest_cpu/runs/rapidstest_cpu_1559329102_76a4e0f3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'rapidstest_cpu_1559329102_76a4e0f3',\n",
       " 'target': 'gpu-todrabas',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2019-05-31T18:58:31.723163Z',\n",
       " 'endTimeUtc': '2019-05-31T19:15:34.968262Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'ContentSnapshotId': '5d4be29b-dedc-4f13-81d8-30851a513f2a',\n",
       "  'azureml.git.repository_uri': 'git@github.com:drabastomek/MLADS_RAPIDS.git',\n",
       "  'mlflow.source.git.repoURL': 'git@github.com:drabastomek/MLADS_RAPIDS.git',\n",
       "  'azureml.git.branch': 'devel',\n",
       "  'mlflow.source.git.branch': 'devel',\n",
       "  'azureml.git.commit': '1c5ecbfb982fac904d415f3dd0ecfff403d4c510',\n",
       "  'mlflow.source.git.commit': '1c5ecbfb982fac904d415f3dd0ecfff403d4c510',\n",
       "  'azureml.git.dirty': 'True'},\n",
       " 'runDefinition': {'script': '1_pandasVsRapids_ETL.py',\n",
       "  'arguments': ['--gpu', '0', '--data_dir', '$AZUREML_DATAREFERENCE_data'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpu-todrabas',\n",
       "  'dataReferences': {'data': {'dataStoreName': 'workspaceblobstore',\n",
       "    'mode': 'Mount',\n",
       "    'pathOnDataStore': 'mortgage_2000',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment rapidstest_cpu Environment',\n",
       "   'version': 'Autosave_2019-05-31T18:55:41Z_e1e6a20e',\n",
       "   'python': {'interpreterPath': '/conda/envs/rapids/bin/python',\n",
       "    'userManagedDependencies': True,\n",
       "    'condaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}],\n",
       "     'channels': ['conda-forge']},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'rapidsai/rapidsai:cuda9.2-runtime-ubuntu18.04',\n",
       "    'enabled': True,\n",
       "    'sharedVolumes': True,\n",
       "    'gpuSupport': True,\n",
       "    'shmSize': '1g',\n",
       "    'arguments': [],\n",
       "    'baseImageRegistry': {'address': None,\n",
       "     'username': None,\n",
       "     'password': None}},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False}},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'vmPriority': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None},\n",
       " 'logFiles': {'azureml-logs/80_driver_log.txt': 'https://todrabasmladsw6357972490.blob.core.windows.net/azureml/ExperimentRun/dcid.rapidstest_cpu_1559329102_76a4e0f3/azureml-logs/80_driver_log.txt?sv=2018-03-28&sr=b&sig=pLZuNGOeEdI97%2BnUXnwddmdtYZ7ROTkYOaFCCtbq%2FdQ%3D&st=2019-05-31T19%3A05%3A36Z&se=2019-06-01T03%3A15%3A36Z&sp=r',\n",
       "  'azureml-logs/55_batchai_stdout-job_prep.txt': 'https://todrabasmladsw6357972490.blob.core.windows.net/azureml/ExperimentRun/dcid.rapidstest_cpu_1559329102_76a4e0f3/azureml-logs/55_batchai_stdout-job_prep.txt?sv=2018-03-28&sr=b&sig=SN6NfPv2uVLBKs%2BDN7uF24QBW5xHJfTLgJFCljDxI4o%3D&st=2019-05-31T19%3A05%3A36Z&se=2019-06-01T03%3A15%3A36Z&sp=r',\n",
       "  'azureml-logs/55_batchai_execution.txt': 'https://todrabasmladsw6357972490.blob.core.windows.net/azureml/ExperimentRun/dcid.rapidstest_cpu_1559329102_76a4e0f3/azureml-logs/55_batchai_execution.txt?sv=2018-03-28&sr=b&sig=UM48qKe%2B1l8nDMy%2B%2FJQnp43AiZuZn%2BKit81pq7sN5pA%3D&st=2019-05-31T19%3A05%3A36Z&se=2019-06-01T03%3A15%3A36Z&sp=r',\n",
       "  'azureml-logs/55_batchai_stdout-job_post.txt': 'https://todrabasmladsw6357972490.blob.core.windows.net/azureml/ExperimentRun/dcid.rapidstest_cpu_1559329102_76a4e0f3/azureml-logs/55_batchai_stdout-job_post.txt?sv=2018-03-28&sr=b&sig=O3Tf9GhUvUpsnrkVvYNPCbzlTG4pCZx8O%2FqpYF4ATgA%3D&st=2019-05-31T19%3A05%3A36Z&se=2019-06-01T03%3A15%3A36Z&sp=r'}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = ScriptRunConfig(source_directory=scripts_folder, \n",
    "                          script='1_pandasVsRapids_ETL.py', \n",
    "                          arguments = ['--gpu', 0, '--data_dir', str(data_ref)\n",
    "                                      ],\n",
    "                          run_config=run_config\n",
    "                         )\n",
    "\n",
    "exp = Experiment(ws, 'rapidstest_cpu')\n",
    "run = exp.submit(config=src)\n",
    "# RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete the cluster\n",
    "gpu_cluster.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
